{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gab Daos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from article_parser import get_covid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\article_parser.py:84: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat(dfs,ignore_index=True)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'ncov_parsed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a77ff69739b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_covid_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\article_parser.py\u001b[0m in \u001b[0;36mget_covid_counts\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ncov_parsed.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'ncov_parsed.csv'"
     ]
    }
   ],
   "source": [
    "get_covid_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>LDA_Topics</th>\n",
       "      <th>count_docs</th>\n",
       "      <th>PH_Loc</th>\n",
       "      <th>status</th>\n",
       "      <th>counts</th>\n",
       "      <th>case</th>\n",
       "      <th>Loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, category, date, source_id, source_name, text, title, LDA_Topics, count_docs, PH_Loc, status, counts, case, Loc]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['counts'] == '1300\\n\\nConfirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-15-379a12db214a>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-379a12db214a>\"\u001b[1;36m, line \u001b[1;32m90\u001b[0m\n\u001b[1;33m    return df\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('pre_processed_data.csv')\n",
    "\n",
    "# Filtering tables to those with PH Locations identified -----------------------\n",
    "\n",
    "df = df[df['counts'] != '1300\\r\\n\\r\\nConfirmed']\n",
    "df = df[df['Loc'] != '']\n",
    "\n",
    "\n",
    "\n",
    "df['counts'] = df.counts.astype(int)\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Parse the results ------------------------------------------------------\n",
    "def parse(df):\n",
    "    #print(df.info())\n",
    "\n",
    "    # Get min/max/mean values\n",
    "    dfa = pd.pivot_table(df, values = 'counts', index=['date', 'Loc'], columns='status', aggfunc=[min, max, np.mean, stats.mode])\n",
    "\n",
    "    # Remove multi-index\n",
    "    dfa.columns = [\"_\".join(pair) for pair in dfa.columns]\n",
    "    dfa = dfa.reset_index()\n",
    "\n",
    "    # Replace 0 with np.nan to forward fill null values\n",
    "    dfa = dfa.replace(0, np.nan)\n",
    "\n",
    "    # Forward filling needs to be by area\n",
    "    places = list(df['Loc'].unique())\n",
    "\n",
    "    global dfb\n",
    "    dfb = pd.DataFrame()\n",
    "    for place in places:\n",
    "        df_temp = dfa[dfa['Loc'] == place].fillna(method='ffill')\n",
    "        dfb = dfb.append(df_temp)\n",
    "    return dfb\n",
    "\n",
    "res = parse(df)\n",
    "\n",
    "# Cleaning results -----------------------------------------------------\n",
    "\n",
    "res = res[['date','Loc', 'min_suspected','min_confirmed']]\n",
    "res = res.fillna(0)\n",
    "res = res.sort_values('date')\n",
    "\n",
    "# Completing running total per date -------------------------------------\n",
    "\n",
    "def add_row(df, row):\n",
    "    df.loc[-1] = row\n",
    "    df.index = df.index + 1  \n",
    "    return df.sort_index()\n",
    "\n",
    "for Loc in res['Loc']:\n",
    "    sus_holder = 0\n",
    "    con_holder = 0\n",
    "    for date in res['date']:\n",
    "        if sum(res[res['date'] == date]['Loc'].str.contains(Loc)) > 0:\n",
    "\n",
    "            sus_holder = res[(res['date'] == date) & (res['Loc'] == Loc)]['min_suspected'].iloc[0]\n",
    "            con_holder = res[(res['date'] == date) & (res['Loc'] == Loc)]['min_confirmed'].iloc[0]\n",
    "        else:\n",
    "            add_row(res, [date, Loc, sus_holder, con_holder])\n",
    "\n",
    "\n",
    "res = res.sort_values('date')\n",
    "\n",
    "# Including the geolocations of each location -------------------------------------\n",
    "\n",
    "prov = gpd.read_file('prov_shp/prov_geo.shp')\n",
    "df = pd.merge(res, prov, left_on = 'Loc', right_on = 'Pro_Name')\n",
    "\n",
    "df['lat'] = [cor.split(',')[0] for cor in df['centroid']]\n",
    "df['long'] = [cor.split(',')[1] for cor in df['centroid']]\n",
    "\n",
    "# Cleaning final results -------------------------------------------------------\n",
    "\n",
    "df = df[['date','Loc','min_suspected','min_confirmed','long','lat']]\n",
    "df.columns = (['Date','Location','Suspected','Confirmed','Longitude','Latitude'])\n",
    "\n",
    "df['Date'] = [datetime.datetime.strptime(str(date), '%Y-%m-%d').strftime('%Y-%m-%dT%H:%M:%S.%f') for date in df['Date']]\n",
    "\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "df.to_csv('ncov_parsed.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mt():    \n",
    "    user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0)'\n",
    "    page_url = 'https://www.manilatimes.net/?s=coronavirus'\n",
    "    manila_times_news = []\n",
    "\n",
    "    df_list = pd.read_csv('articles_list/m_times_articles_urls.csv', header = None);\n",
    "    article_list = df_list[0]\n",
    "\n",
    "    while page_url != False:\n",
    "        req = Request(page_url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "            'Accept-Encoding': 'none',\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'Connection': 'keep-alive'})\n",
    "        content = urlopen(req).read()\n",
    "        soup = BeautifulSoup(content)\n",
    "        res = (soup.find_all('a',{'rel':\"bookmark\"}))\n",
    "\n",
    "        #manila_times_news += [url['href'] for url in res]\n",
    "        for url in res:\n",
    "            if url['href'] in article_list.values:\n",
    "                page_url = False\n",
    "                print(url['href'] + ' stopped latest')\n",
    "                break\n",
    "            else:\n",
    "                manila_times_news.append(url['href'])\n",
    "\n",
    "        if page_url == False:\n",
    "            break\n",
    "\n",
    "        date = (soup.find_all('time'))\n",
    "\n",
    "        page_link = soup.find('div',{'class':\"page-nav\"} )\n",
    "\n",
    "        time.sleep(3)\n",
    "        if date[0]['datetime'] < '2020-01-20':\n",
    "            page_url = False\n",
    "        else:\n",
    "            page_url =  page_link.find_all('a')[-1]['href']\n",
    "            print(page_url)\n",
    "\n",
    "    mt_df = pd.DataFrame(columns = ['source_id','date','category','title','author','text'])\n",
    "    df = pd.read_csv('scraped_data/manila_times_scraped.csv')\n",
    "\n",
    "    try:    \n",
    "        for article in manila_times_news:  \n",
    "            user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "            req = Request(article, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'})\n",
    "            content = urlopen(req).read()\n",
    "            soup = BeautifulSoup(content)\n",
    "\n",
    "            try:\n",
    "                article_id = ''\n",
    "\n",
    "                date = soup.find('time').text\n",
    "\n",
    "                category = 'nCov'\n",
    "                title = soup.find('h1', {'class' : 'tdb-title-text'}).text\n",
    "                author = soup.find('a', {'class' : 'tdb-author-name'}).text\n",
    "\n",
    "                text = soup.find('div', {'class' : 'td-post-content'})\n",
    "                text.find_all('p')\n",
    "\n",
    "                paragraph = \"\"\n",
    "                for x in text.find_all('p'):\n",
    "                    paragraph += x.text.strip() + \" \" \n",
    "\n",
    "                text = paragraph\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "            if (title in df['title'].values):\n",
    "                print('Reached latest article')\n",
    "                break\n",
    "\n",
    "            print(title, date)\n",
    "            mt_df = mt_df.append(pd.Series([article_id,date,category, title,author, text], index = mt_df.columns ), ignore_index=True)\n",
    "            time.sleep(2)\n",
    "\n",
    "        df = df.append(mt_df)\n",
    "        df.to_csv('scraped_data/manila_times_scraped.csv', index = False)\n",
    "\n",
    "        df_list[0].append(manila_times_news).reset_index(drop = True).to_csv('articles_list/m_times_articles_urls.csv', index = False)\n",
    "\n",
    "    except:\n",
    "        df = df.append(mt_df)\n",
    "        df.to_csv('scraped_data/manila_times_scraped.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.manilatimes.net/2020/03/15/news/latest-stories/ferrari-suspends-production-over-coronavirus/703354/ stopped latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:89: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "get_mt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.Series(manila_times_news).to_csv('articles_list/m_times_articles_url.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = pd.read_csv('articles_list/m_times_articles_url.csv', header = None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ferrari suspends production over coronavirus March 15, 2020\n",
      "British football shuts down to halt coronavirus spread March 15, 2020\n",
      "Raptors test negative for coronavirus March 15, 2020\n",
      "Augusta National postpones Masters golf due to coronavirus March 15, 2020\n",
      "Coronavirus world: Science and math key to managing March 15, 2020\n",
      "Supply and demand, hoarding, price gouging — and the coronavirus March 15, 2020\n",
      "India and the coronavirus: Lucky escape or emergency ahead? March 15, 2020\n",
      "Joining Hands in Coronavirus Combat: Policy Recommendations March 14, 2020\n",
      "Filipino diplomat at UN coronavirus-infected March 14, 2020\n",
      "Coronavirus-infected people forbidden to enter mosques March 14, 2020\n",
      "NBA coronavirus shutdown likely to be ‘at least 30 days’ – Silver March 14, 2020\n",
      "Bong Go backs Congress special session to OK Coronavirus budget March 14, 2020\n",
      "LRT-1 maintains normal operating schedule March 15, 2020\n",
      "Spain imposes near total lockdown to fight virus March 15, 2020\n",
      "Ferrari suspends production over coronavirus March 15, 2020\n",
      "Italy’s health workers on edge of exhaustion in virus fight March 15, 2020\n",
      "Raging virus makes zero rates a possibility as Fed meets March 15, 2020\n",
      "NBA suspends season after Jazz player tests positive for coronavirus March 13, 2020\n",
      "Juventus defender Rugani tests positive for coronavirus March 13, 2020\n",
      "Most coronavirus patients recover, still anxiety, fear loom March 12, 2020\n",
      "NBA suspends season until further notice, over coronavirus March 12, 2020\n",
      "Tom Hanks announces positive test for coronavirus March 12, 2020\n",
      "Palace warns arrest of people taking advantage of coronavirus situation March 12, 2020\n",
      "What China Contributes to the Global Coronavirus Combat March 12, 2020\n",
      "THE CORONAVIRUS SHACKLE’S EFFECT March 12, 2020\n",
      "Flu and coronavirus: Similar symptoms, different fears March 11, 2020\n",
      "Coronavirus clusters swell on both sides of the US March 11, 2020\n",
      "UN: Coronavirus crisis could cost world up to $2 trillion March 11, 2020\n",
      "PBA to push through amid coronavirus? March 11, 2020\n",
      "LRT-1 maintains normal operating schedule March 15, 2020\n",
      "Spain imposes near total lockdown to fight virus March 15, 2020\n",
      "Ferrari suspends production over coronavirus March 15, 2020\n",
      "Italy’s health workers on edge of exhaustion in virus fight March 15, 2020\n",
      "Raging virus makes zero rates a possibility as Fed meets March 15, 2020\n",
      "Stocks plummet amid coronavirus fears and oil-price crash March 10, 2020\n",
      "Coronavirus: US plane diverted after passengers upset by sneezing March 10, 2020\n",
      "NKorea, SKorea vow to beat coronavirus March 9, 2020\n",
      "Coronavirus outbreak to drag sentiment March 9, 2020\n",
      "Dealing with the coronavirus with a 50-year-old budget process March 8, 2020\n",
      "US allots $8.3B to fight coronavirus March 7, 2020\n",
      "WHO chief warns against complacency in novel coronavirus fight March 7, 2020\n",
      "Coronavirus threatens to wreck nuclear review conference March 6, 2020\n",
      "Panic buying follows coronavirus across the globe March 5, 2020\n",
      "Coronavirus: When not to whoop it up March 4, 2020\n",
      "Wobbling Juve, Inter aim for cup glory amid coronavirus March 4, 2020\n",
      "Pope ‘tests negative for coronavirus’ – report March 3, 2020\n",
      "LRT-1 maintains normal operating schedule March 15, 2020\n",
      "Spain imposes near total lockdown to fight virus March 15, 2020\n",
      "Ferrari suspends production over coronavirus March 15, 2020\n",
      "Italy’s health workers on edge of exhaustion in virus fight March 15, 2020\n",
      "Raging virus makes zero rates a possibility as Fed meets March 15, 2020\n",
      "South Korea declares ‘war’ on coronavirus as cases approach 5,000 March 3, 2020\n",
      "CORONAVIRUS’ TOP TARGETS ARE MEN, SENIORS AND SMOKERS…. March 3, 2020\n",
      "Coronavirus reduces pollution in China — NASA March 3, 2020\n",
      "How wrong assumptions helped coronavirus to spread in Europe March 3, 2020\n",
      "In the eye of the coronavirus storm March 2, 2020\n",
      "The revenge of Nature — coronavirus? March 1, 2020\n",
      "Coronavirus coincidences made to speak for themselves March 1, 2020\n",
      "Coronavirus coincidences made to speak for themselves March 1, 2020\n",
      "Cebuana Lhuillier remittances hit by coronavirus February 28, 2020\n",
      "NKorea imposes ‘extraordinary’ measures against coronavirus February 28, 2020\n",
      "New US coronavirus case may be 1st from unknown origin February 27, 2020\n",
      "Trump mulls travel bans on Italy, SKorea over coronavirus February 27, 2020\n",
      "LRT-1 maintains normal operating schedule March 15, 2020\n",
      "Spain imposes near total lockdown to fight virus March 15, 2020\n",
      "Ferrari suspends production over coronavirus March 15, 2020\n",
      "Italy’s health workers on edge of exhaustion in virus fight March 15, 2020\n",
      "Raging virus makes zero rates a possibility as Fed meets March 15, 2020\n",
      "Reached latest article\n"
     ]
    }
   ],
   "source": [
    "mt_df = pd.DataFrame(columns = ['source_id','date','category','title','author','text'])\n",
    "df = pd.read_csv('scraped_data/manila_times_scraped.csv')\n",
    "try:    \n",
    "    for article in articles_list:  \n",
    "        user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "        req = Request(article, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "            'Accept-Encoding': 'none',\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'Connection': 'keep-alive'})\n",
    "        content = urlopen(req).read()\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        try:\n",
    "            article_id = ''\n",
    "\n",
    "            date = soup.find('time').text\n",
    "\n",
    "            category = 'nCov'\n",
    "            title = soup.find('h1', {'class' : 'tdb-title-text'}).text\n",
    "            author = soup.find('a', {'class' : 'tdb-author-name'}).text\n",
    "\n",
    "            text = soup.find('div', {'class' : 'td-post-content'})\n",
    "            text.find_all('p')\n",
    "\n",
    "            paragraph = \"\"\n",
    "            for x in text.find_all('p'):\n",
    "                paragraph += x.text.strip() + \" \" \n",
    "\n",
    "            text = paragraph\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "        if (title in df['title'].values):\n",
    "            print('Reached latest article')\n",
    "            break\n",
    "\n",
    "        print(title, date)\n",
    "        mt_df = mt_df.append(pd.Series([article_id,date,category, title,author, text], index = mt_df.columns ), ignore_index=True)\n",
    "        time.sleep(2)\n",
    "\n",
    "    df = df.append(mt_df)\n",
    "    df.to_csv('scraped_data/new_manila_times_scraped.csv', index = False)\n",
    "except:\n",
    "    df = df.append(mt_df)\n",
    "    df.to_csv('scraped_data/new_manila_times_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_df_holder = mt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_df.to_csv('scraped_data/manila_times_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "req = Request(article, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "'Accept-Encoding': 'none',\n",
    "'Accept-Language': 'en-US,en;q=0.8',\n",
    "'Connection': 'keep-alive'})\n",
    "content = urlopen(req).read()\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "article_id = ''\n",
    "\n",
    "date = soup.find('time').text\n",
    "\n",
    "category = 'nCov'\n",
    "title = soup.find('h1', {'class' : 'tdb-title-text'}).text\n",
    "author = soup.find('a', {'class' : 'tdb-author-name'}).text\n",
    "\n",
    "text = soup.find('div', {'class' : 'td-post-content'})\n",
    "text.find_all('p')\n",
    "\n",
    "paragraph = \"\"\n",
    "for x in text.find_all('p'):\n",
    "    paragraph += x.text.strip() + \" \" \n",
    "\n",
    "text = paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_data/abscbn_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Australia confirms 1st coronavirucases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = list(df['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test in article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_scrapers import get_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:175: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 175 of the file C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(content)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "https://news.mb.com.ph/tag/ncov/page1/\n",
      "https://news.mb.com.ph/2020/03/15/medical-practitioner-is-2nd-confirmed-covid-19-case-in-cavite/ stopped latest\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:199: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 199 of the file C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(content)\n",
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:262: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  df_list[0].append(to_add).reset_index(drop = True).to_csv('articles_list/mb_ncov_articles_urls.csv', index = False)\n"
     ]
    }
   ],
   "source": [
    "get_mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
