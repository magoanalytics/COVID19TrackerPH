{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gab Daos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from article_parser import get_covid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\article_parser.py:84: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat(dfs,ignore_index=True)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'ncov_parsed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a77ff69739b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_covid_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\article_parser.py\u001b[0m in \u001b[0;36mget_covid_counts\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ncov_parsed.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'ncov_parsed.csv'"
     ]
    }
   ],
   "source": [
    "get_covid_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>LDA_Topics</th>\n",
       "      <th>count_docs</th>\n",
       "      <th>PH_Loc</th>\n",
       "      <th>status</th>\n",
       "      <th>counts</th>\n",
       "      <th>case</th>\n",
       "      <th>Loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, category, date, source_id, source_name, text, title, LDA_Topics, count_docs, PH_Loc, status, counts, case, Loc]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['counts'] == '1300\\n\\nConfirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-15-379a12db214a>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-379a12db214a>\"\u001b[1;36m, line \u001b[1;32m90\u001b[0m\n\u001b[1;33m    return df\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('pre_processed_data.csv')\n",
    "\n",
    "# Filtering tables to those with PH Locations identified -----------------------\n",
    "\n",
    "df = df[df['counts'] != '1300\\r\\n\\r\\nConfirmed']\n",
    "df = df[df['Loc'] != '']\n",
    "\n",
    "\n",
    "\n",
    "df['counts'] = df.counts.astype(int)\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Parse the results ------------------------------------------------------\n",
    "def parse(df):\n",
    "    #print(df.info())\n",
    "\n",
    "    # Get min/max/mean values\n",
    "    dfa = pd.pivot_table(df, values = 'counts', index=['date', 'Loc'], columns='status', aggfunc=[min, max, np.mean, stats.mode])\n",
    "\n",
    "    # Remove multi-index\n",
    "    dfa.columns = [\"_\".join(pair) for pair in dfa.columns]\n",
    "    dfa = dfa.reset_index()\n",
    "\n",
    "    # Replace 0 with np.nan to forward fill null values\n",
    "    dfa = dfa.replace(0, np.nan)\n",
    "\n",
    "    # Forward filling needs to be by area\n",
    "    places = list(df['Loc'].unique())\n",
    "\n",
    "    global dfb\n",
    "    dfb = pd.DataFrame()\n",
    "    for place in places:\n",
    "        df_temp = dfa[dfa['Loc'] == place].fillna(method='ffill')\n",
    "        dfb = dfb.append(df_temp)\n",
    "    return dfb\n",
    "\n",
    "res = parse(df)\n",
    "\n",
    "# Cleaning results -----------------------------------------------------\n",
    "\n",
    "res = res[['date','Loc', 'min_suspected','min_confirmed']]\n",
    "res = res.fillna(0)\n",
    "res = res.sort_values('date')\n",
    "\n",
    "# Completing running total per date -------------------------------------\n",
    "\n",
    "def add_row(df, row):\n",
    "    df.loc[-1] = row\n",
    "    df.index = df.index + 1  \n",
    "    return df.sort_index()\n",
    "\n",
    "for Loc in res['Loc']:\n",
    "    sus_holder = 0\n",
    "    con_holder = 0\n",
    "    for date in res['date']:\n",
    "        if sum(res[res['date'] == date]['Loc'].str.contains(Loc)) > 0:\n",
    "\n",
    "            sus_holder = res[(res['date'] == date) & (res['Loc'] == Loc)]['min_suspected'].iloc[0]\n",
    "            con_holder = res[(res['date'] == date) & (res['Loc'] == Loc)]['min_confirmed'].iloc[0]\n",
    "        else:\n",
    "            add_row(res, [date, Loc, sus_holder, con_holder])\n",
    "\n",
    "\n",
    "res = res.sort_values('date')\n",
    "\n",
    "# Including the geolocations of each location -------------------------------------\n",
    "\n",
    "prov = gpd.read_file('prov_shp/prov_geo.shp')\n",
    "df = pd.merge(res, prov, left_on = 'Loc', right_on = 'Pro_Name')\n",
    "\n",
    "df['lat'] = [cor.split(',')[0] for cor in df['centroid']]\n",
    "df['long'] = [cor.split(',')[1] for cor in df['centroid']]\n",
    "\n",
    "# Cleaning final results -------------------------------------------------------\n",
    "\n",
    "df = df[['date','Loc','min_suspected','min_confirmed','long','lat']]\n",
    "df.columns = (['Date','Location','Suspected','Confirmed','Longitude','Latitude'])\n",
    "\n",
    "df['Date'] = [datetime.datetime.strptime(str(date), '%Y-%m-%d').strftime('%Y-%m-%dT%H:%M:%S.%f') for date in df['Date']]\n",
    "\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "df.to_csv('ncov_parsed.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.manilatimes.net/page/2/?s=coronavirus\n",
      "https://www.manilatimes.net/page/3/?s=coronavirus\n",
      "https://www.manilatimes.net/page/4/?s=coronavirus\n",
      "https://www.manilatimes.net/page/5/?s=coronavirus\n",
      "https://www.manilatimes.net/page/6/?s=coronavirus\n",
      "https://www.manilatimes.net/page/7/?s=coronavirus\n",
      "https://www.manilatimes.net/page/8/?s=coronavirus\n",
      "https://www.manilatimes.net/page/9/?s=coronavirus\n",
      "https://www.manilatimes.net/page/10/?s=coronavirus\n",
      "https://www.manilatimes.net/page/11/?s=coronavirus\n",
      "https://www.manilatimes.net/page/12/?s=coronavirus\n",
      "https://www.manilatimes.net/page/13/?s=coronavirus\n",
      "https://www.manilatimes.net/page/14/?s=coronavirus\n",
      "https://www.manilatimes.net/page/15/?s=coronavirus\n",
      "https://www.manilatimes.net/page/16/?s=coronavirus\n",
      "https://www.manilatimes.net/page/17/?s=coronavirus\n",
      "https://www.manilatimes.net/page/18/?s=coronavirus\n",
      "https://www.manilatimes.net/page/19/?s=coronavirus\n",
      "https://www.manilatimes.net/page/20/?s=coronavirus\n",
      "https://www.manilatimes.net/page/21/?s=coronavirus\n",
      "https://www.manilatimes.net/page/22/?s=coronavirus\n",
      "https://www.manilatimes.net/page/23/?s=coronavirus\n",
      "https://www.manilatimes.net/page/24/?s=coronavirus\n",
      "https://www.manilatimes.net/page/25/?s=coronavirus\n",
      "https://www.manilatimes.net/page/26/?s=coronavirus\n",
      "https://www.manilatimes.net/page/27/?s=coronavirus\n",
      "https://www.manilatimes.net/page/28/?s=coronavirus\n",
      "https://www.manilatimes.net/page/29/?s=coronavirus\n",
      "https://www.manilatimes.net/page/30/?s=coronavirus\n",
      "https://www.manilatimes.net/page/31/?s=coronavirus\n",
      "https://www.manilatimes.net/page/32/?s=coronavirus\n",
      "https://www.manilatimes.net/page/33/?s=coronavirus\n",
      "https://www.manilatimes.net/page/34/?s=coronavirus\n",
      "https://www.manilatimes.net/page/35/?s=coronavirus\n",
      "https://www.manilatimes.net/page/36/?s=coronavirus\n",
      "https://www.manilatimes.net/page/37/?s=coronavirus\n",
      "https://www.manilatimes.net/page/38/?s=coronavirus\n",
      "https://www.manilatimes.net/page/39/?s=coronavirus\n",
      "https://www.manilatimes.net/page/40/?s=coronavirus\n",
      "https://www.manilatimes.net/page/41/?s=coronavirus\n",
      "https://www.manilatimes.net/page/42/?s=coronavirus\n",
      "https://www.manilatimes.net/page/43/?s=coronavirus\n",
      "https://www.manilatimes.net/page/44/?s=coronavirus\n",
      "https://www.manilatimes.net/page/45/?s=coronavirus\n",
      "https://www.manilatimes.net/page/46/?s=coronavirus\n",
      "https://www.manilatimes.net/page/47/?s=coronavirus\n",
      "https://www.manilatimes.net/page/48/?s=coronavirus\n",
      "https://www.manilatimes.net/page/49/?s=coronavirus\n",
      "https://www.manilatimes.net/page/50/?s=coronavirus\n",
      "https://www.manilatimes.net/page/51/?s=coronavirus\n",
      "https://www.manilatimes.net/page/52/?s=coronavirus\n"
     ]
    }
   ],
   "source": [
    "user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0)'\n",
    "page_url = 'https://www.manilatimes.net/?s=coronavirus'\n",
    "manila_times_news = []\n",
    "\n",
    "while page_url != False:\n",
    "    req = Request(page_url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'})\n",
    "    content = urlopen(req).read()\n",
    "    soup = BeautifulSoup(content)\n",
    "    res = (soup.find_all('a',{'rel':\"bookmark\"}))\n",
    "\n",
    "    manila_times_news += [url['href'] for url in res]\n",
    "    date = (soup.find_all('time'))\n",
    "    \n",
    "    page_link = soup.find('div',{'class':\"page-nav\"} )\n",
    "    \n",
    "    time.sleep(6)\n",
    "    if date[0]['datetime'] < '2020-01-20':\n",
    "        page_url = False\n",
    "    else:\n",
    "        page_url =  page_link.find_all('a')[-1]['href']\n",
    "        print(page_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.Series(manila_times_news).to_csv('m_times_articles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('m_times_articles.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.manilatimes.net/2020/01/31/news/regions/mother-daughter-under-watch-for-ncov-in-nueva-vizcaya/678582/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-ef8c24a09577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;34m'Accept-Encoding'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;34m'Accept-Language'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'en-US,en;q=0.8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         'Connection': 'keep-alive'})\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    326\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                  method=None):\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mfull_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown url type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: 'h'"
     ]
    }
   ],
   "source": [
    "mt_df = pd.DataFrame(columns = ['source_id','date','category','title','author','text'])\n",
    "for article in df['links']:  \n",
    "    user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "    req = Request(article, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'})\n",
    "    content = urlopen(req).read()\n",
    "    soup = BeautifulSoup(content)\n",
    "\n",
    "    try:\n",
    "        article_id = ''\n",
    "\n",
    "        date = soup.find('time').text\n",
    "\n",
    "        category = 'nCov'\n",
    "        title = soup.find('h1', {'class' : 'tdb-title-text'}).text\n",
    "        author = soup.find('a', {'class' : 'tdb-author-name'}).text\n",
    "\n",
    "        text = soup.find('div', {'class' : 'td-post-content'})\n",
    "        text.find_all('p')\n",
    "\n",
    "        paragraph = \"\"\n",
    "        for x in text.find_all('p'):\n",
    "            paragraph += x.text.strip() + \" \" \n",
    "\n",
    "        text = paragraph\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "    print(title, date)\n",
    "    mt_df = mt_df.append(pd.Series([article_id,date,category, title,author, text], index = mt_df.columns ), ignore_index=True)\n",
    "    time.sleep(1)\n",
    "    \n",
    "#mt_df.to_csv('scraped_data/manila_times_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_df_holder = mt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_df.to_csv('scraped_data/manila_times_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "req = Request(article, headers={'User-Agent': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "'Accept-Encoding': 'none',\n",
    "'Accept-Language': 'en-US,en;q=0.8',\n",
    "'Connection': 'keep-alive'})\n",
    "content = urlopen(req).read()\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "article_id = ''\n",
    "\n",
    "date = soup.find('time').text\n",
    "\n",
    "category = 'nCov'\n",
    "title = soup.find('h1', {'class' : 'tdb-title-text'}).text\n",
    "author = soup.find('a', {'class' : 'tdb-author-name'}).text\n",
    "\n",
    "text = soup.find('div', {'class' : 'td-post-content'})\n",
    "text.find_all('p')\n",
    "\n",
    "paragraph = \"\"\n",
    "for x in text.find_all('p'):\n",
    "    paragraph += x.text.strip() + \" \" \n",
    "\n",
    "text = paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_data/abscbn_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Australia confirms 1st coronavirucases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = list(df['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test in article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_scrapers import get_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:175: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 175 of the file C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(content)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "https://news.mb.com.ph/tag/ncov/page1/\n",
      "https://news.mb.com.ph/2020/03/15/medical-practitioner-is-2nd-confirmed-covid-19-case-in-cavite/ stopped latest\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:199: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 199 of the file C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(content)\n",
      "C:\\Users\\Gab Daos\\Documents\\Mago\\Git Projects\\COVID19TrackerPH\\gab\\news_scrapers.py:262: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  df_list[0].append(to_add).reset_index(drop = True).to_csv('articles_list/mb_ncov_articles_urls.csv', index = False)\n"
     ]
    }
   ],
   "source": [
    "get_mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
